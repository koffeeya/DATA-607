---
title: "DATA 607, Project 3: The Most Valued Data Science Skills"
date: "March 25, 2018"
output:
  html_document:
    theme: yeti
    highlight: haddock
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr>

### Intro

Start with a spam/ham dataset, then predict the class of new documents withheld from the training dataset.

Set up our work space
```{r, echo = F}
# Background stuff - tweak and conceal as needed

# Clean up our workspace
rm(list = ls())

# Identify the workbench
wd.path1 <- "C:/Users/jlobr/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
wd.path2 <- "G:/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
```

```{r}
# Go to the library - thank you open source!
library(tm)
library(RTextTools)
library(dplyr)
library(ggplot2)
library(magrittr)
library(stringr)

# Hygiene factor
setwd(file.path(wd.path2, "Projects", "Project 4"))
```

### Approach

The goal is to prepare document data for classification algorithms.  We'll use the tm and RTextTools packages to so, proceeding step by step.

[I don't think we'll have time to use tidy approach to text mining, and weshould probably just keep it to the "tm" package for this assignment]


**Step: Read in spam and ham datasets.**
**Step: Clean the data**
**Step: Create spam corpus, ham corpus, and full (combined) corpus (I think)**
**Step: Label which are spam and ham by their source**
**Step: Create TDM and DTM (if both necessary)**
**Step: Create train and test sets (if necessary, may be built into other step)**
**Step: Create container**
**Step: Extract text from spam and ham documents (if necessary)**
**Step: Run classifier algorithms**
**Step: Enumerate conclusions**


**Step: Read in spam and ham datasets.**
```{r}
# Message sets were downloaded from spamassassin's archive to a local folder and unzippedper the tutorial
# !!! Could attempt to implement and automate this in R if time permits) !!!

# 
spam.directory <- file.path(getwd(), "spamundham", "spam")
spam.list <- list.files(spam.directory)
spam <- readLines(spam.directory, spam.list)
str_c(collapse = "") %>% 
Corpus(VectorSource())

# ham.directory <- file.path(getwd(), "spamundham", "easy_ham")
# ham.list <- list.files(ham.directory)

# Create spam, ham, and combined (full) corpuses
spam.corpus <- Corpus(DirSource(spam.directory))
ham.corpus <- Corpus(DirSource(ham.directory))

# !!! tm_combine function not found. This step isn't working and I think we'll a need a combined corpus for the classifiers.  TBD !!! #
full.corpus <- tm_combine(cv(spam.corpus, ham.corpus)
```


**Step: Clean the data**
**Step: Create spam corpus, ham corpus, and full (combined) corpus (I think)**
**Step: Label which are spam and ham by their source**
```{r}
**Step: Clean the data**
# Clean each corpus, changing all text to lower case and removing punctuation, stopwords, and numbers.
# !!! Annotate with "what for" !!!
# !!! If we preserve this approach, credit azabet:  https://github.com/azabet/Machine-Learning/blob/master/spam-filter.R !!!
# !!! Review cleaning procedures to assess whether these steps are necessary / other steps are needed !!!
# str_replace_all(pattern = "<.*?>", replacement = " ") %>% 
# str_replace_all(pattern = "\\=", replacement = "") %>%
# !!! If possible, replace spam and ham corpuses with combined corpus that includes labels.

spam.corpus.clean <- spam.corpus %>%
  tm_map(tolower) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, stopwords('english')) %>% 
  tm_map(removeNumbers) # %>% 
  # tm_map(stripWhitespace) 
  # tm_map(stemDocument, language = "english")

ham.corpus.clean<- ham.corpus %>%
  tm_map(tolower) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, stopwords('english')) %>% 
  tm_map(removeNumbers) # %>% 
  # tm_map(stripWhitespace) %>% 
  # tm_map(stemDocument, language = "english")

full.corpus.clean <- spam.corpus %>%
  tm_map(tolower) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, stopwords('english')) %>% 
  tm_map(removeNumbers) # %>% 
  # tm_map(stripWhitespace) 
  # tm_map(stemDocument, language = "english")

# Inspect cleaned corpuses for sense-check
inspect(spam.corpus.clean[1])
inspect(ham.corpus.clean[1])

# Create a label vector for ham and spam, or full
# !!! Think this step may need to occure prior, and that this syntax may not be right !!!
sh_labels <- prescindMeta(ham.corpus.clean)
```


**Step: Creat TDM and DTM (if both necessary)**
```{r}
# Create Document-Term Matrices for ham and spam corpuses
# !!! These don't look right... !!!
ham.dtm <- DocumentTermMatrix(ham.corpus.clean)
spam.dtm <- DocumentTermMatrix(spam.corpus.clean)

# Create Term-Document Matrices for ham and spam corpuses
# !!! These don't look right... !!!
ham.tdm <- TermDocumentMatrix(ham.corpus.clean)
spam.tdm <- TermDocumentMatrix(spam.corpus.clean)

# Check output
# !!! Look into whether there's anything to say about this sparsity !!!
inspect(ham.dtm)
inspect(spam.dtm)

# Remove some sparsity
# !!! Not sure if this step will help !!!
ham.tdm.rich <- removeSparseTerms(ham.tdm, (1 - 10 / length(ham.corpus.clean)))
spam.dtm.rich <- removeSparseTerms(spam.tdm, (1 - 10 / length(spam.corpus.clean)))
ham.tdm.rich
spam.dtm.rich
```


**Step: Create train and test sets (if necessary, may be built into other step)**
```{r}
# Partition training and test sets
# We can take two approaches - one that splits up at the file level, another that splits at the corpus level.  Corpus is implemented below, as it seems to make for simpler approach to controlling sampling rate.
# !!! Implement once on each spam and ham, or just on full? !!!

# Corpus-oriented approach to sampling to partition for training and test sets
# This is based on a samping rate holdour of 10% for the test set.  We can tweak as we see appropriate - perhaps there's a benchmark rate?
training = .9 # set training between 0 and 1 as sampling rate

# !!! We may be able to perform this more simply using arguments in create_containter !!!

# First, split the cleaned spam corpus into train and test
spam.n <- length(spam.corpus.clean)
spam.n.test <- spam.n * (1 - training)
spam.test.cut <- sample(1:spam.n, size = spam.n.test, replace = F)
spam.train.cut <- setdiff(1:spam.n, spam.test.cut)
spam.train <- spam.corpus.clean[spam.train.cut]
spam.test <- spam.corpus.clean[spam.test.cut]

# Next, split the cleaned ham corpus into train and test
ham.n <- length(ham.corpus.clean)
ham.n.test <- ham.n * (1 - training)
ham.test.cut <- sample(1:ham.n, size = ham.n.test, replace = F)
ham.train.cut <- setdiff(1:ham.n, ham.test.cut)
ham.train <- ham.corpus.clean[ham.train.cut]
ham.test <- ham.corpus.clean[ham.test.cut]
```


**Step: Extract text from spam and ham documents.**
```{r}
# Is include this approach, credit spam filtering code snippet in the "Spam Filtering" chapter Machine Learning for Hackers around page 80.
# !!! We may not need this prior or not at all.  TBD !!! #
extract.msg = function(msg) {
  con = file(msg, open = "rt", encoding = "latin1")
  text = readLines(con)
  msg = text[seq(which(text == "")[1] + 1, length(text),1)]
  close(con)
  return(paste(msg, collapse = "\\n"))
}
```


**Step: Create container**
```{r}
# container <- create_container(full.corpus.clean, 
                              #labels = sh.labels, 
                              #trainSize = 1:?, 
                              #testSize = ?:n,
                              # virgin = F)
```


**Step: Run classifier algorithms**
```{r}
# svm <- train_model(container, "SVM")
# tree <- train_model(container, "TREE")
# maxent <- train_model(container, "MAXENT")
```


### Conclusions



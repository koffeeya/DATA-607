---
title: "DATA 607, Project 3: The Most Valued Data Science Skills"
date: "March 25, 2018"
output:
  html_document:
    theme: yeti
    highlight: haddock
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr>


### Intro

[Regurgitate project parameters]

```{r, echo = F}
# Background stuff - tweak and conceal as needed

# Clean up our workspace
rm(list = ls())

# Identify the workbench
wd.path <- "C:/Users/jlobr/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
```


```{r}
# Go to the library - thank you open source!
library(tm)
library(RTextTools)
library(dplyr)
library(ggplot2)

# Hygiene factor
setwd(file.path(wd.path, "Projects", "Project 4"))
getwd()
```

Approach

[Describe goal]
[Outline approach]

Load files in spam and ham folder
Create cuts of training and test data

```{r}
setwd()

# Message sets were downloaded from spamassassin's archive to a local folder and unzippedper the tutorial (could attempt to implement and automate this in R if time permits)

# Set directories
spam.directory <- file.path(getwd(), "spamundham", "spam")
ham.directory <- file.path(getwd(), "spamundham", "easy_ham")

# Set data (may not be necessary depending on corpus setup)
spam.list <- list.files(spam.directory)
ham.list <- list.files(ham.directory)

# Create corpsuses
# Credit azabet:  https://github.com/azabet/Machine-Learning/blob/master/spam-filter.R
spam.corpus <- Corpus(DirSource(spam.directory))
ham.corpus <- Corpus(DirSource(ham.directory))

# Cleaning
# Credit azabet:  https://github.com/azabet/Machine-Learning/blob/master/spam-filter.R
# Review cleaning procedures to assess whether these steps are necessary / other steps are needed
# If there's time, loop for spam and ham labels to de-duplicate code
spam.corpus.clean <- spam.corpus %>%
  tm_map(tolower) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, stopwords('english')) %>% 
  tm_map(removeNumbers)
ham.corpus.clean<- ham.corpus %>%
  tm_map(tolower) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, stopwords('english')) %>% 
  tm_map(removeNumbers)

# Partition training and test sets
# We can take two approaches - one the splits up at the file level, another that splits at the corpus level.  Corpus is implemented below, as it seems to make for simpler approach to controlling sampling rate.
# What should the sampling rate be based on n?  Holdout of 10% for test?
# Implement once on each folder

# Corpus-oriented approach to sampling to partition for training and test sets
training = .9 # set training between 0 and 1 as sampling rate

# First, split the cleaned spam corpus into train and test
spam.n <- length(spam.corpus.clean)
spam.n.test <- spam.n * (1 - training)
spam.test.cut <- sample(1:spam.n, size = spam.n.test, replace = F)
spam.train.cut <- setdiff(1:spam.n, spam.test.cut)   # setdiff apparently available in base R
spam.train <- spam.corpus.clean[spam.train.cut]
spam.test <- spam.corpus.clean[spam.test.cut]

# Next, split the cleaned ham corpus into train and test
ham.n <- length(ham.corpus.clean)
ham.n.test <- ham.n * (1 - training)
ham.test.cut <- sample(1:ham.n, size = ham.n.test, replace = F)
ham.train.cut <- setdiff(1:ham.n, ham.test.cut)
ham.train <- ham.corpus.clean[ham.train.cut]
ham.test <- ham.corpus.clean[ham.test.cut]
```

Extract text from spam and ham documents
```{r}
# Found a spam filtering code snippet in the "Spam Filtering" chapter Machine Learning for Hackers around page 80.
extract.msg = function(msg) {
  con = file(msg, open = "rt", encoding = "latin1")
  text = readLines(con)
  msg = text[seq(which(text == "")[1] + 1, length(text),1)]
  close(con)
  return(paste(msg, collapse = "\\n"))
}

```


Create term document matrix

Build classifier use training data

Use tidy or just tm?

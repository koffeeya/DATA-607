---
title: "DATA 607, Project 4"
date: "March 25, 2018"
output:
  html_document:
    theme: yeti
    highlight: haddock
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr>

# Background

### A. Purpose

The purpose of this assignment.....

<br>

### B. Approach

1. Read in the data.

2. Create a corpus.
   
3. Clean the data.

4. Create a document-term matrix.

5. Create a container.

6. Train the data.

7. Evaluate the results.

<br>

### B. Libraries and Setup

```{r, echo = F}

# Clear the workspace
rm(list = ls())

# Identify the workbench (in case of Jeremy's laptop or desktop)
#wd.path1 <- "C:/Users/jlobr/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
wd.path2 <- "G:/OneDrive/Learning/_CUNY_SPS_MSDS/2018_Spring/DATA 607/"
```


```{r message=FALSE, warning=FALSE}
library(tm)
library(RTextTools)
library(dplyr)
library(ggplot2)
library(magrittr)
library(stringr)
library(tidyr)
library(SnowballC)
```

<br>

****

# 1. Read in the data.

### A. Set the working directory to be the location of the extracted ham and spam folders.

```{r}

# Identify the working directory - we worked collaboratively but with local files, so we flipped this switch when iterating

# Kavya WD
#setwd <- "C:/Users/Kavya/Desktop/Education/msds/DATA 607 Data Acquisition and Management/Projects/Project 04"

# Jeremy's WD
setwd(file.path(wd.path2, "Projects", "Project 4"))
```

<br>

### B. Create separate lists of spam and ham filepaths.

```{r}

# The name of the folders in our working directories -- in this case, "spam" and "ham" -- become the `path` argument below.

ham.list <- list.files(path = "spam/", full.names = T, recursive = F)

spam.list <- list.files(path = "ham/", full.names = T, recursive = F)

```

<br>

### C. Apply the `readLines` function to every filepath to get the contents of the file (i.e., the email itself).

```{r}

ham.sapply <- sapply(ham.list, readLines, warn = F)

spam.sapply <- sapply(spam.list, readLines, warn = F)

```

<br>

****

# 2. Create a corpus

### A. Combine the two lists of files and create a dataframe.

```{r, message = F}

# Build a dataframe composed of the text read in from the ham and spam emails.

combined1 <- c(ham.sapply, spam.sapply)

combined2 <- data.frame(t(sapply(combined1,c)))

combined.df <- gather(combined2, "file", "text", 1:2796)

# Coerce the text variable to character type 

combined.df$text <- as.character(combined.df$text)

```

<br>

### B. Add a column for "type" and label the documents as spam or ham.

```{r}

# Prime the type variable

combined.df$type <- NA

# Set ranges to map ham and spam values to row ranges within the dataframe

ham.l <- length(ham.sapply)

combined.l <- nrow(combined.df)

# Map the appropriate values to type in the dataframe

combined.df$type[1:ham.l] <- "spam"

combined.df$type[(ham.l + 1):(combined.l)] <- "ham"

combined.df$type <- factor(combined.df$type)

```

<br>

### C. Perform some basic cleaning.  We elected not to remove html tags using regex to turn "<[^>]*>"" into blanks, as that level of formatting could conceivably signal whether an email is ham of spam.

```{r}

combined.df$text <-  str_replace_all(combined.df$text, "^c\\(", "") %>% # Remove 
                     str_replace_all("[[:punct:]]", " ") %>% # Remove punctuation
                     str_replace_all("\\s{2,}", " ") # Truncate white space (to single space)

combined.df$text <- enc2utf8(combined.df$text)

```

<br>

### D. Create a corpus

```{r}

combined.corpus <- Corpus(VectorSource(combined.df$text))

combined.corpus <- sample(combined.corpus)

print(combined.corpus)

```

****

<br>

# 3. Create a document-term matrix.

```{r}

corpus.dtm <- DocumentTermMatrix(combined.corpus)

corpus.dtm <- removeSparseTerms(corpus.dtm, (1 - 10 / length(combined.corpus)))

print(corpus.dtm)

```

****

<br>

# 4. Create a container.

### A. Set the parameters.  We'd like to see if different train-test proportion impact predictive power of the classifiers, so we'll create a few different parameters.

```{r}

# Specify the location of the "spam" and "ham" labels

labels.corpus <- combined.df$type

# The total number of documents -- 2,796

N <- length(combined.corpus)

# The percentage of the data to partition
P.70 <- 0.7 # For a 30% test holdout
P.80 <- 0.8 # For a 20% test holdout
P.90 <- 0.9 # For a 10% test holdout

# Number of documents in the training set
trainSize.70 <- round(P.70*N)
trainSize.80 <- round(P.80*N)
trainSize.90 <- round(P.90*N)

# Number of documents in the test (holdout) set
testSize.70 <- N - round(P.70*N+1)
testSize.80 <- N - round(P.80*N+1)
testSize.90 <- N - round(P.90*N+1)

paste0("For a 30% test holdout, training set = ", trainSize.70, " and test set = ", testSize.70)
paste0("For a 20% test holdout, training set = ", trainSize.80, " and test set = ", testSize.80)
paste0("For a 10% test holdout, training set = ", trainSize.90, " and test set = ", testSize.90)
```

<br>

### B. Create the containers - one each for each of the test holdout levels.

```{r}

# Create a container for the 30% test holdout
container.70 <- create_container(corpus.dtm, 
                              labels = labels.corpus, 
                              trainSize = 1:trainSize.70, 
                              testSize = (trainSize.70+1):N,
                              virgin = F)

slotNames(container.70)


# Create containers for the 20% and 10% test holdouts
container.80 <- create_container(corpus.dtm, 
                              labels = labels.corpus, 
                              trainSize = 1:trainSize.80, 
                              testSize = (trainSize.80+1):N,
                              virgin = F)

container.90 <- create_container(corpus.dtm, 
                              labels = labels.corpus, 
                              trainSize = 1:trainSize.90, 
                              testSize = (trainSize.90+1):N,
                              virgin = F)

```


****

<br>

# 5. Train the data - again, at different test holdout levels.  We used three classifiers: SVM, Random Forest, and Maximum Entropy models.  The Random Forest model crashed every machine we tried, so we discontinued that approach.

```{r}
# Classify using the Support Vector Machines model
svm_model.70 <- train_model(container.70, "SVM")
svm_out.70 <- classify_model(container.70, svm_model)

svm_model.80 <- train_model(container.80, "SVM")
svm_out.80 <- classify_model(container.80, svm_model)

svm_model.90 <- train_model(container.90, "SVM")
svm_out.90 <- classify_model(container.90, svm_model)

# Classify using the Maximum Entropy model
maxent_model.70 <- train_model(container.70, "MAXENT")
maxent_out.70 <- classify_model(container.70, maxent_model)

maxent_model.80 <- train_model(container.80, "MAXENT")
maxent_out.80 <- classify_model(container.80, maxent_model)

maxent_model.90 <- train_model(container.90, "MAXENT")
maxent_out.90 <- classify_model(container.90, maxent_model)


```

****

<br>

# 6. Evaluate the results.

### A. Create a new dataframe with the true and expected classifications.

```{r}

labels.out.70 <- data.frame( 
  correct.label = labels.corpus[round(P.70*N+1):N], 
  svm.70 = as.character(svm_out.70[,1]), 
  #tree = as.character(tree_out[,1]), 
  maxent.70 = as.character(maxent_out.70[,1]), 
  stringsAsFactors = F)

labels.out.80 <- data.frame( 
  correct.label = labels.corpus[round(P.80*N+1):N], 
  svm.80 = as.character(svm_out.80[,1]), 
  #tree = as.character(tree_out[,1]), 
  maxent.80 = as.character(maxent_out.80[,1]), 
  stringsAsFactors = F)

labels.out.90 <- data.frame( 
  correct.label = labels.corpus[round(P.90*N+1):N], 
  svm.90 = as.character(svm_out.90[,1]), 
  #tree = as.character(tree_out[,1]), 
  maxent.90 = as.character(maxent_out.90[,1]), 
  stringsAsFactors = F)

head(labels.out.70)

```

<br>

### B. Performance of Support Vector Machine Model at different test holdout levels.

```{r}

table(labels.out.70[,1] == labels.out.70[,2]) 
prop.table(table(labels.out.70[,1] == labels.out.70[,2])) 

table(labels.out.80[,1] == labels.out.80[,2]) 
prop.table(table(labels.out.80[,1] == labels.out.80[,2])) 

table(labels.out.90[,1] == labels.out.90[,2]) 
prop.table(table(labels.out.90[,1] == labels.out.90[,2])) 

```

<br>

### C. Performance of Maximum Entropy Model at different test holdout levels.

```{r}

table(labels.out.70[,1] == labels.out.70[,3])
prop.table(table(labels.out.70[,1] == labels.out.70[,3])) 

table(labels.out.80[,1] == labels.out.80[,3])
prop.table(table(labels.out.80[,1] == labels.out.80[,3])) 

table(labels.out.90[,1] == labels.out.90[,3])
prop.table(table(labels.out.90[,1] == labels.out.90[,3])) 

```

<br>

****

# 7. Evaluation

### Results from the SVM classifier are puzzlingly accurate -- 100%.  This does not vary based on the size of the training and test sets.  We are skeptical, and feel this would benefit from further debugging.

### Results from the Maximum Entropy Model were also puzzling, but for a different reason.  Accuracy seemed to decline as the training set grew in size, which runs counter to our intuitions.

